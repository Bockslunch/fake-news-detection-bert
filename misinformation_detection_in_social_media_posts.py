# -*- coding: utf-8 -*-
"""Copy of Misinformation Detection in Social Media Posts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QVvcbD_rhL_Rz9HO0tmlnh37lXiKIQnY

Fake News Classification with BERT and LIME for Interpretability
- Uses BERT model for text classification
- Implements LIME for model explanations
- Memory-optimized for Google Colab GPU
"""

# Import libraries
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, BertForSequenceClassification, BertConfig
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.metrics import roc_auc_score, classification_report, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
import re
import time
from tqdm.notebook import tqdm

# Interpretability tool - LIME only (Couldn't get Captum to work)
!pip install lime
import lime
import lime.lime_text

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Dataset Loading
print("Please upload the four FakeNewsNet CSV files: gossipcop_fake.csv, gossipcop_real.csv, politifact_fake.csv, politifact_real.csv")
uploaded = files.upload()

# Load and label the datasets
df_gossipcop_fake = pd.read_csv('gossipcop_fake.csv', sep=',', on_bad_lines='skip')
df_gossipcop_fake['label'] = 1  # Fake
df_gossipcop_real = pd.read_csv('gossipcop_real.csv', sep=',', on_bad_lines='skip')
df_gossipcop_real['label'] = 0  # Real
df_politifact_fake = pd.read_csv('politifact_fake.csv', sep=',', on_bad_lines='skip')
df_politifact_fake['label'] = 1  # Fake
df_politifact_real = pd.read_csv('politifact_real.csv', sep=',', on_bad_lines='skip')
df_politifact_real['label'] = 0  # Real

# Combine all datasets into one DataFrame
df = pd.concat([df_gossipcop_fake, df_gossipcop_real, df_politifact_fake, df_politifact_real], ignore_index=True)

# Preprocessing: Use 'title' column, remove NaN and short texts
df = df.dropna(subset=['title'])  # Drop rows with missing titles
df = df[df['title'].str.len() >= 10]  # Filter out titles shorter than 10 characters
df = df[['title', 'label']]  # Keep only relevant columns
print(f"Dataset size after filtering: {len(df)}")
print(f"Class distribution:\n{df['label'].value_counts()}")

# Display a few examples
print("\nSample data:")
print(df.sample(5))

# Data Preparation and Model Setup
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class NewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.raw_texts = [str(text) for text in texts]  # Store raw texts for interpretability

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long),
            'raw_text': self.raw_texts[idx]
        }

# Split dataset: 80% train, 10% validation, 10% test with stratification
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    df['title'].values, df['label'].values, test_size=0.2, stratify=df['label'], random_state=SEED
)
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=SEED
)

print(f"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}")

# Create datasets
train_dataset = NewsDataset(train_texts, train_labels, tokenizer)
val_dataset = NewsDataset(val_texts, val_labels, tokenizer)
test_dataset = NewsDataset(test_texts, test_labels, tokenizer)

# Create data loaders
batch_size = 8  # Reduced batch size to avoid OOM errors
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Load BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model = model.to(device)

# Use AdamW optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

# Training function
def train_epoch(model, data_loader, optimizer, device, scheduler=None):
    model.train()
    total_loss = 0

    # Free up memory
    torch.cuda.empty_cache()

    progress_bar = tqdm(data_loader, desc="Training")
    for batch in progress_bar:
        # Zero gradients
        optimizer.zero_grad()

        # Move batch to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()
        if scheduler:
            scheduler.step()

        # Update statistics
        total_loss += loss.item()
        progress_bar.set_postfix({'loss': loss.item()})

        # Free up memory
        del input_ids, attention_mask, labels, outputs, loss
        torch.cuda.empty_cache()

    return total_loss / len(data_loader)

# Evaluation function
def evaluate(model, data_loader, device):
    model.eval()
    preds, true_labels, probs = [], [], []
    raw_texts = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()

            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
            probs.extend(probabilities)

            # Store raw texts for interpretability
            if 'raw_text' in batch:
                raw_texts.extend(batch['raw_text'])

            # Free memory
            del input_ids, attention_mask, labels, outputs, logits
            torch.cuda.empty_cache()

    metrics = {
        'accuracy': accuracy_score(true_labels, preds),
        'f1': f1_score(true_labels, preds),
        'precision': precision_score(true_labels, preds),
        'recall': recall_score(true_labels, preds),
        'auc_roc': roc_auc_score(true_labels, probs)
    }

    return preds, true_labels, probs, raw_texts, metrics

# Train the model for 3 epochs
print("Starting training...")
num_epochs = 3
train_losses = []
val_metrics_history = []

for epoch in range(num_epochs):
    start_time = time.time()

    # Train for one epoch
    train_loss = train_epoch(model, train_loader, optimizer, device)
    train_losses.append(train_loss)

    # Evaluate on validation set
    val_preds, val_true, val_probs, val_texts, val_metrics = evaluate(model, val_loader, device)
    val_metrics_history.append(val_metrics)

    epoch_time = time.time() - start_time

    print(f"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s")
    print(f"Training Loss: {train_loss:.4f}")
    print(f"Validation Metrics: Accuracy={val_metrics['accuracy']:.4f}, F1={val_metrics['f1']:.4f}, AUC={val_metrics['auc_roc']:.4f}")

# Step 4: Evaluate on validation set and generate confusion matrix
test_preds, test_true, test_probs, test_texts, test_metrics = evaluate(model, test_loader, device)

print("\nFinal Test Metrics:")
for metric_name, metric_value in test_metrics.items():
    print(f"{metric_name.capitalize()}: {metric_value:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(test_true, test_preds, target_names=['Real', 'Fake']))

# Confusion matrix visualization
cm = confusion_matrix(test_true, test_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(test_true, test_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.tight_layout()
plt.savefig('roc_curve.png')
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(test_true, test_probs)
pr_auc = auc(recall, precision)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'PR curve (area = {pr_auc:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.tight_layout()
plt.savefig('pr_curve.png')
plt.show()

# Model Interpretability with Lime

def bert_pipeline_predict(texts):
    """
    Pipeline prediction function for LIME
    """
    model.eval()
    all_probs = []

    for text in texts:
        encoding = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=128,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()

        all_probs.append(probs[0])

        # Free memory
        del input_ids, attention_mask, outputs
        torch.cuda.empty_cache()

    return np.array(all_probs)

def analyze_with_lime(text, bert_pipeline_predict, class_names=['Real', 'Fake'], num_features=10):
    """
    Analyze a text sample with LIME
    """
    print(f"Text: {text}")

    # Create LIME explainer
    explainer = lime.lime_text.LimeTextExplainer(class_names=class_names)

    # Get explanation
    explanation = explainer.explain_instance(
        text,
        lambda x: bert_pipeline_predict(x),
        num_features=num_features,
        num_samples=500
    )

    # Get prediction
    probs = bert_pipeline_predict([text])[0]
    predicted_class = np.argmax(probs)

    print(f"Prediction: {class_names[predicted_class]} (Confidence: {probs[predicted_class]:.4f})")

    # Get feature weights
    weights = explanation.as_list()

    # Sort weights by absolute value for feature importance ranking
    weights.sort(key=lambda x: abs(x[1]), reverse=True)

    # Get top features and their weights
    features = [x[0] for x in weights[:num_features]]
    feature_weights = [x[1] for x in weights[:num_features]]

    # Create colors based on positive or negative contribution
    colors = ['green' if w > 0 else 'red' for w in feature_weights]

    # Create bar chart for feature importance ranking
    plt.figure(figsize=(10, 6))
    y_pos = range(len(features))
    plt.barh(y_pos, feature_weights, color=colors)
    plt.yticks(y_pos, features)
    plt.xlabel('Weight (Green = Contributes to Real, Red = Contributes to Fake)')
    plt.title(f'Top {num_features} Words Influencing Prediction: {class_names[predicted_class]}')
    plt.tight_layout()
    plt.savefig('lime_feature_importance.png')
    plt.show()

    # Create text highlighting visualization
    try:
        plt.figure(figsize=(10, 3))
        explanation.show_in_notebook(text=True, predict_proba=False)
        plt.title(f'Word Highlighting - Prediction: {class_names[predicted_class]}')
        plt.tight_layout()
        plt.savefig('lime_text_highlight.png')
        plt.show()
    except Exception as e:
        print(f"Could not generate text highlighting visualization: {str(e)}")

    # Print feature importance ranking table
    print("\nFeature Importance Ranking:")
    for i, (word, weight) in enumerate(weights[:num_features], 1):
        impact = "positive (Real)" if weight > 0 else "negative (Fake)"
        print(f"{i}. \"{word}\" - Impact: {impact}, Weight: {weight:.4f}")

    return explanation

# Demonstrate Interpretability
print("\n===== MODEL INTERPRETABILITY DEMONSTRATIONS =====")

# Select examples for interpretation
def get_examples_for_interpretation(texts, labels, num_samples=2):
    """Get examples from each class for interpretation"""
    fake_indices = np.where(labels == 1)[0]
    real_indices = np.where(labels == 0)[0]

    # Get random samples from each class
    np.random.seed(SEED)
    fake_samples = np.random.choice(fake_indices, min(num_samples, len(fake_indices)), replace=False)
    real_samples = np.random.choice(real_indices, min(num_samples, len(real_indices)), replace=False)

    selected_indices = np.concatenate([fake_samples, real_samples])
    selected_texts = [texts[i] for i in selected_indices]
    selected_labels = [labels[i] for i in selected_indices]

    return selected_texts, selected_labels, selected_indices

# Get examples
selected_texts, selected_labels, selected_indices = get_examples_for_interpretation(test_texts, test_labels)

# LIME Analysis
print("\n--- LIME Analysis ---")
for i, (text, label) in enumerate(zip(selected_texts, selected_labels)):
    print(f"\nExample {i+1} (True Class: {'Fake' if label == 1 else 'Real'}):")
    lime_explanation = analyze_with_lime(
        text,
        bert_pipeline_predict,
        class_names=['Real', 'Fake'],
        num_features=10  # Show top 10 most important words
    )
    print("\n" + "-"*50)

# Save Model
model_path = "bert_misinfo_model_with_interpretability"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)
print(f"\nModel saved to {model_path}")

# Conclusion
print("\n===== MODEL EVALUATION SUMMARY =====")
print(f"Accuracy: {test_metrics['accuracy']:.4f}")
print(f"F1 Score: {test_metrics['f1']:.4f}")
print(f"AUC-ROC: {test_metrics['auc_roc']:.4f}")
print("\nModel interpretability implemented with LIME:")
print("- Feature importance ranking visualization")
print("- Word contribution highlighting")
print("- Text predictions with importance scores")
print("\nAll visualizations saved as PNG files")
print("\nKey benefits of LIME for fake news detection:")
print("1. Transparency: Understand why the model classifies news as real or fake")
print("2. Debugging: Identify potential biases or overfitting in the model")
print("3. Trust: Build confidence in the model's decisions through clear explanations")
print("4. Improvement: Refine the model based on insights from important features")















